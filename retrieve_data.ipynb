{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "## Load the NASA_API_KEY from the env file\n",
    "load_dotenv()\n",
    "NASA_API_KEY = os.getenv(\"NASA_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CME Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base URL to NASA's DONKI API:\n",
    "base_url = \"https://api.nasa.gov/DONKI/\"\n",
    "\n",
    "# Set the specifier for CMEs:\n",
    "CME = \"CME\"\n",
    "\n",
    "# Search for CMEs published between a begin and end date\n",
    "startDate = \"2023-05-01\"\n",
    "endDate = \"2024-05-01\"\n",
    "\n",
    "# Build URL for CME\n",
    "query_url = (\n",
    "    f\"{base_url}{CME}?startDate={startDate}&endDate={endDate}&api_key={NASA_API_KEY}\"\n",
    ")\n",
    "print(query_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a \"GET\" request for the CME URL and store it in a variable named cme_response\n",
    "cme_response = requests.get(url=query_url)\n",
    "\n",
    "if cme_response.status_code != 200:\n",
    "    print(f\"Connection Error!! Code: {cme_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the response variable to json and store it as a variable named cme_json\n",
    "cme_json = cme_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview the first result in JSON format\n",
    "# Use json.dumps with argument indent=4 to format data\n",
    "print(json.dumps(cme_json, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cme_json to a Pandas DataFrame\n",
    "df = pd.DataFrame(cme_json)\n",
    "df.info()\n",
    "# Keep only the columns: activityID, startTime, linkedEvents\n",
    "df = df[[\"activityID\", \"startTime\", \"linkedEvents\"]]\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the linkedEvents column allows us to identify the corresponding GST\n",
    "# Remove rows with missing 'linkedEvents' since we won't be able to assign these to GSTs\n",
    "df = df.dropna(how='any')\n",
    "df.isna().sum()\n",
    "# df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the linkedEvents sometimes contains multiple events per row\n",
    "# Write a nested for loop that iterates first over each row in the cme DataFrame (using the index)\n",
    "# and then iterates over the values in 'linkedEvents'\n",
    "# and adds the elements individually to a list of dictionaries where each row is one element\n",
    "\n",
    "# Initialize an empty list to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Iterate over each index in the DataFrame\n",
    "for i in range(len(df)):\n",
    "  \n",
    "    # Iterate over each dictionary in the list\n",
    "    \"\"\"\n",
    "      for loop with iloc\n",
    "    \"\"\"\n",
    "    for e in df.iloc[i][\"linkedEvents\"]:\n",
    "\n",
    "        # Append a new dictionary to the expanded_rows list for each dictionary item and corresponding 'activityID' and 'startTime' value\n",
    "        expanded_rows.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame from the expanded rows\n",
    "events_df = pd.DataFrame(expanded_rows)\n",
    "events_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function called extract_activityID_from_dict that takes a dict as input such as in linkedEvents\n",
    "# and verify below that it works as expected using one row from linkedEvents as an example\n",
    "# Be sure to use a try and except block to handle errors\n",
    "\"\"\"\n",
    "  @params(event: [])\n",
    "  @returns(activityID: [] | read_errors: [])\n",
    "  Extracts and returns the activityId from\n",
    "  linkedEvents dictionary.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_activityID_from_dict(events: list):\n",
    "    ids = []\n",
    "    for e in range(len(events)):\n",
    "        try:\n",
    "            ids.append(events[e][\"activityID\"])\n",
    "        except KeyError:\n",
    "            ids.append(\"ID ERROR!\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this function to each row in the 'linkedEvents' column (you can use apply() and a lambda function)\n",
    "# and create a new column called 'GST_ActivityID' using loc indexer:\n",
    "\n",
    "df[\"GST_ActivityID\"] = df.linkedEvents.apply(\n",
    "    lambda x: extract_activityID_from_dict(x)\n",
    ")\n",
    "\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing GST_ActivityID, since we can't assign them to GSTs:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the datatype of each column in this DataFrame:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 265 entries, 1 to 1227\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype              \n",
      "---  ------          --------------  -----              \n",
      " 0   cmeID           265 non-null    object             \n",
      " 1   startTime_CME   265 non-null    datetime64[ns, UTC]\n",
      " 2   GST_ActivityID  265 non-null    string             \n",
      "dtypes: datetime64[ns, UTC](1), object(1), string(1)\n",
      "memory usage: 16.4+ KB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  NOTE: This cell is difficult to control.\n",
    "  I am going to wrap this in a function so\n",
    "  it can be used more surgically\n",
    "\n",
    "  I am coping the original df because it will\n",
    "  allow the user to either store the copy as\n",
    "  a new variable or save the cdf to the \n",
    "  original df. Making the choice of how \n",
    "  they want to use the result a little\n",
    "  easier.\n",
    "'''\n",
    "def cleanUp(df):\n",
    "    # Copy the original df (clean dataframe).\n",
    "    cdf = df.copy()\n",
    "    \n",
    "    # Convert the 'GST_ActivityID' column to string format \n",
    "    cdf.GST_ActivityID = cdf.GST_ActivityID.astype('string')\n",
    "\n",
    "    if 'startTime' in df.columns:\n",
    "      # Convert startTime to datetime format  \n",
    "      cdf.startTime = pd.to_datetime(df.startTime)\n",
    "\n",
    "      # Rename startTime to startTime_CME and activityID to cmeID\n",
    "      cdf = cdf.rename(columns={'startTime':'startTime_CME', 'activityID':'cmeID'})\n",
    "\n",
    "    # Drop linkedEvents\n",
    "    cdf = cdf.drop('linkedEvents', axis=1)\n",
    "\n",
    "    # Verify that all steps were executed correctly\n",
    "    cdf.info()\n",
    "\n",
    "    # Return cdf so it can be saved as a variable\n",
    "    return cdf\n",
    "\n",
    "clean_df = cleanUp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 265 entries, 1 to 1227\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype              \n",
      "---  ------          --------------  -----              \n",
      " 0   cmeID           265 non-null    object             \n",
      " 1   startTime_CME   265 non-null    datetime64[ns, UTC]\n",
      " 2   GST_ActivityID  265 non-null    string             \n",
      "dtypes: datetime64[ns, UTC](1), object(1), string(1)\n",
      "memory usage: 16.4+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-08-05T02:10:00-IPS-001', '2023-08-05T03:00:00-GST-001', '2023-08-05T04:57:00-IPS-001', '2023-08-09T15:20:00-RBE-001', '2023-08-11T13:05:00-RBE-001']\n",
      "['2023-09-14T21:17:00-FLR-001', '2023-09-18T17:54:00-MPC-001', '2023-09-19T00:00:00-GST-001', '2023-09-19T14:55:00-RBE-001']\n",
      "['2023-09-18T09:04:00-IPS-001', '2023-09-18T12:58:00-IPS-001', '2023-09-18T17:54:00-MPC-001', '2023-09-19T00:00:00-GST-001', '2023-09-19T14:55:00-RBE-001']\n",
      "['2023-11-05T03:38:00-IPS-001', '2023-11-05T08:10:00-IPS-001', '2023-11-05T09:00:00-GST-001', '2023-11-05T10:34:00-MPC-001', '2023-11-07T15:35:00-RBE-001']\n",
      "['2023-11-03T04:40:00-FLR-001', '2023-11-05T09:00:00-GST-001', '2023-11-05T11:45:00-IPS-001', '2023-11-05T12:35:00-IPS-001', '2023-11-05T14:52:00-MPC-001', '2023-11-07T15:35:00-RBE-001']\n",
      "['2023-11-22T18:58:00-FLR-001', '2023-11-25T04:30:00-IPS-001', '2023-11-25T07:59:00-IPS-001', '2023-11-25T09:28:00-MPC-001', '2023-11-25T18:00:00-GST-001']\n",
      "['2023-11-28T19:07:00-FLR-001', '2023-11-28T19:35:00-FLR-001', '2023-12-01T08:15:00-IPS-001', '2023-12-01T08:48:00-IPS-001', '2023-12-01T09:00:00-GST-001', '2023-12-01T11:04:00-MPC-001', '2023-12-02T00:46:00-MPC-001']\n",
      "['2023-11-28T19:07:00-FLR-001', '2023-11-28T19:35:00-FLR-001', '2023-12-01T08:15:00-IPS-001', '2023-12-01T08:48:00-IPS-001', '2023-12-01T09:00:00-GST-001', '2023-12-01T11:04:00-MPC-001', '2023-12-02T00:46:00-MPC-001']\n",
      "['2023-12-14T16:47:00-FLR-001', '2023-12-15T10:05:00-SEP-001', '2023-12-15T14:57:00-SEP-001', '2023-12-15T15:42:00-SEP-001', '2023-12-15T23:45:00-SEP-001', '2023-12-17T00:30:00-IPS-001', '2023-12-17T07:32:00-IPS-001', '2023-12-17T10:54:00-MPC-001', '2023-12-18T06:00:00-GST-001', '2023-12-21T13:35:00-RBE-001']\n",
      "['2024-02-28T16:24:00-FLR-001', '2024-03-03T05:58:00-IPS-001', '2024-03-03T08:47:00-IPS-001', '2024-03-03T18:00:00-GST-001']\n",
      "['2024-03-23T00:58:00-FLR-001', '2024-03-23T05:57:00-SEP-001', '2024-03-23T06:49:00-SEP-001', '2024-03-23T08:15:00-SEP-001', '2024-03-23T09:55:00-SEP-001', '2024-03-24T12:00:00-GST-001', '2024-03-24T14:10:00-IPS-001', '2024-03-24T14:27:00-IPS-001', '2024-03-24T16:25:00-MPC-001']\n",
      "['2024-03-24T12:00:00-GST-001', '2024-03-24T14:10:00-IPS-001', '2024-03-24T14:27:00-IPS-001', '2024-03-24T16:25:00-MPC-001']\n",
      "['2024-04-15T05:47:00-FLR-001', '2024-04-19T04:53:00-IPS-001', '2024-04-19T18:00:00-GST-001', '2024-04-24T15:45:00-RBE-001']\n"
     ]
    }
   ],
   "source": [
    "# We are only interested in CMEs related to GSTs so keep only rows where the GST_ActivityID column contains 'GST'\n",
    "# use the method 'contains()' from the str library.\n",
    "\n",
    "'''\n",
    "  save all strings that contain 'GST'\n",
    "'''\n",
    "gst_events = clean_df[clean_df.GST_ActivityID.str.contains('GST')]\n",
    "\n",
    "'''\n",
    "  loop through the events and print each event\n",
    "'''\n",
    "for event in gst_events.GST_ActivityID:\n",
    "  print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base URL to NASA's DONKI API:\n",
    "base_url = \"https://api.nasa.gov/DONKI/\"\n",
    "\n",
    "# Set the specifier for Geomagnetic Storms (GST):\n",
    "GST = \"GST\"\n",
    "\n",
    "# Search for GSTs between a begin and end date\n",
    "startDate = \"2013-05-01\"\n",
    "endDate   = \"2024-05-01\"\n",
    "\n",
    "# Build URL for GST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a \"GET\" request for the GST URL and store it in a variable named gst_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the response variable to json and store it as a variable named gst_json\n",
    "\n",
    "# Preview the first result in JSON format\n",
    "# Use json.dumps with argument indent=4 to format data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gst_json to a Pandas DataFrame  \n",
    "\n",
    "# Keep only the columns: activityID, startTime, linkedEvents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the linkedEvents column allows us to identify the corresponding CME\n",
    "# Remove rows with missing 'linkedEvents' since we won't be able to assign these to CME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the linkedEvents sometimes contains multiple events per row\n",
    "# Use the explode method to ensure that each row is one element. Ensure to reset the index and drop missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extract_activityID_from_dict function to each row in the 'linkedEvents' column (you can use apply() and a lambda function)\n",
    "# and create a new column called 'CME_ActivityID' using loc indexer:\n",
    "\n",
    "# Remove rows with missing CME_ActivityID, since we can't assign them to CMEs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'CME_ActivityID' column to string format \n",
    "\n",
    "# Convert the 'gstID' column to string format \n",
    "\n",
    "# Convert startTime to datetime format  \n",
    "\n",
    "# Rename startTime to startTime_GST \n",
    "\n",
    "# Drop linkedEvents\n",
    "\n",
    "# Verify that all steps were executed correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in GSTs related to CMEs so keep only rows where the CME_ActivityID column contains 'CME'\n",
    "# use the method 'contains()' from the str library.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge both datatsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now merge both datasets using 'gstID' and 'CME_ActivityID' for gst and 'GST_ActivityID' and 'cmeID' for cme. Use the 'left_on' and 'right_on' specifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the new DataFrame has the same number of rows as cme and gst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the time it takes for a CME to cause a GST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the time diff between startTime_GST and startTime_CME by creating a new column called `timeDiff`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use describe() to compute the mean and median time \n",
    "# that it takes for a CME to cause a GST. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV without the index\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
